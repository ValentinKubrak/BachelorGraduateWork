{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkW5z5i0HwsL"
      },
      "outputs": [],
      "source": [
        "# Завантаження бібліотек\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "import cv2\n",
        "import time\n",
        "from numba import njit # numba для прискорення математичних операцій"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfJjc7H8Xb9N"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "# Відключення попереджень про застарілість\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oarLnRTlHxs1"
      },
      "outputs": [],
      "source": [
        "np.random.seed(777)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOzXwQlXHy46"
      },
      "outputs": [],
      "source": [
        "# Функція активації ReLU\n",
        "@njit(fastmath=True)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0a4tHnyTn6n"
      },
      "outputs": [],
      "source": [
        "# Функція активації Softmax\n",
        "@njit(fastmath=True)\n",
        "def softmax(raw_preds):\n",
        "    out = np.exp(raw_preds)\n",
        "    return out/np.sum(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0qz0WvtzyZn"
      },
      "outputs": [],
      "source": [
        "#original\n",
        "def convolution_layer(input_value, kernel, bias, stride=1, padding=0):\n",
        "    padded_image = np.pad(input_value, ((0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
        "    (n_f, n_c_f, f, _) = kernel.shape\n",
        "    n_c, in_dim, _ = padded_image.shape\n",
        "\n",
        "    out_dim = int((in_dim - f)/stride)+1\n",
        "\n",
        "    out = np.zeros((n_f,out_dim,out_dim))\n",
        "\n",
        "    for curr_f in range(n_f):\n",
        "        curr_y = out_y = 0\n",
        "        while curr_y + f <= in_dim:\n",
        "            curr_x = out_x = 0\n",
        "            while curr_x + f <= in_dim:\n",
        "                out[curr_f, out_y, out_x] = np.sum(kernel[curr_f] * padded_image[:,curr_y:curr_y+f, curr_x:curr_x+f]) + bias[curr_f]\n",
        "                curr_x += stride\n",
        "                out_x += 1\n",
        "            curr_y += stride\n",
        "            out_y += 1\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Шар згортки\n",
        "def conv_layer(input_tensor, filter_weights, filter_bias, step=1, pad=0):\n",
        "    \"\"\"\n",
        "    Застосовує шар згортки до вхідного тензора\n",
        "\n",
        "    Параметри:\n",
        "    input_tensor - Вхідний тензор.\n",
        "    filter_weights - Ваги фільтрів.\n",
        "    filter_bias - Зміщення фільтрів.\n",
        "    step - Крок згортки. За замовчуванням 1.\n",
        "    pad - Розмір краю для padding. За замовчуванням 0.\n",
        "\n",
        "    Повертає:\n",
        "    np.array - Результат застосування шару згортки до вхідного тензора.\n",
        "    \"\"\"\n",
        "    # padding вхідного тензора\n",
        "    padded_tensor = np.pad(input_tensor, ((0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
        "    # Отримання розмірностей\n",
        "    (num_filters, num_channels_filt, filt_size, _) = filter_weights.shape\n",
        "    num_channels, input_size, _ = padded_tensor.shape\n",
        "    # Розрахунок розмірності вихідного тензора\n",
        "    output_size = int((input_size - filt_size) / step) + 1\n",
        "    # Ініціалізація вихідного тензора\n",
        "    output_tensor = np.zeros((num_filters, output_size, output_size))\n",
        "\n",
        "    # Проходження фільтрами та розрахунок вихідних значень\n",
        "    for filter_index in range(num_filters):\n",
        "        input_y = output_y = 0\n",
        "        while input_y + filt_size <= input_size:\n",
        "            input_x = output_x = 0\n",
        "            while input_x + filt_size <= input_size:\n",
        "                output_tensor[filter_index, output_y, output_x] = np.sum(filter_weights[filter_index] * padded_tensor[:, input_y:input_y+filt_size, input_x:input_x+filt_size]) + filter_bias[filter_index]\n",
        "                input_x += step\n",
        "                output_x += 1\n",
        "            input_y += step\n",
        "            output_y += 1\n",
        "\n",
        "    return output_tensor"
      ],
      "metadata": {
        "id": "j_KNQgek97iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krPGzJPSYtLD"
      },
      "outputs": [],
      "source": [
        "# Шар максимального пулінгу\n",
        "def max_pooling(input_value, pool_size=2, stride=2):\n",
        "    # Визначення розмірів вхідного значення\n",
        "    input_depth, input_height, input_width = input_value.shape\n",
        "    # Розрахунок висоти та ширини вихідного значення\n",
        "    output_height = (input_height - pool_size) // stride + 1\n",
        "    output_width = (input_width - pool_size) // stride + 1\n",
        "    # Ініціалізація вихідного значення нулями\n",
        "    output_value = np.zeros((input_depth, output_height, output_width))\n",
        "\n",
        "    # Перебір по всіх глибинах вхідного значення\n",
        "    for d in range(input_depth):\n",
        "        # Перебір по висоті вхідного значення з урахуванням кроку\n",
        "        for i in range(0, input_height - pool_size + 1, stride):\n",
        "            # Перебір по ширині вхідного значення з урахуванням кроку\n",
        "            for j in range(0, input_width - pool_size + 1, stride):\n",
        "                # Обчислення максимального значення в поточному підвікні\n",
        "                output_value[d, i // stride, j // stride] = np.max(\n",
        "                    input_value[d, i:i + pool_size, j:j + pool_size])\n",
        "    # Повернення вихідного значення\n",
        "    return output_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rrY9IwiokUN"
      },
      "outputs": [],
      "source": [
        "# Функція втрат - категорійна крос-ентропія\n",
        "@njit(fastmath=True)\n",
        "def cross_entropy_loss(predicted, target):\n",
        "    return -np.sum(target * np.log(predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y39nLE2e283E"
      },
      "outputs": [],
      "source": [
        "# original\n",
        "@njit(fastmath=True)\n",
        "def convolutionBackward(dconv_prev, conv_in, filt, s):\n",
        "    (n_f, n_c, f, _) = filt.shape\n",
        "    (_, orig_dim, _) = conv_in.shape\n",
        "\n",
        "    dout = np.zeros(conv_in.shape)\n",
        "    dfilt = np.zeros(filt.shape)\n",
        "    dbias = np.zeros((n_f,1))\n",
        "    for curr_f in range(n_f):\n",
        "\n",
        "        curr_y = out_y = 0\n",
        "        while curr_y + f <= orig_dim:\n",
        "            curr_x = out_x = 0\n",
        "            while curr_x + f <= orig_dim:\n",
        "\n",
        "                dfilt[curr_f] += dconv_prev[curr_f, out_y, out_x] * conv_in[:, curr_y:curr_y+f, curr_x:curr_x+f]\n",
        "\n",
        "                dout[:, curr_y:curr_y+f, curr_x:curr_x+f] += dconv_prev[curr_f, out_y, out_x] * filt[curr_f]\n",
        "                curr_x += s\n",
        "                out_x += 1\n",
        "            curr_y += s\n",
        "            out_y += 1\n",
        "\n",
        "        dbias[curr_f] = np.sum(dconv_prev[curr_f])\n",
        "\n",
        "    return dout, dfilt, dbias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Зворотнє розповсюдження помилки для шару згортки\n",
        "@njit(fastmath=True)\n",
        "def conv_backward(d_out, input_tensor, filters, stride):\n",
        "    \"\"\"\n",
        "    Обчислює зворотнє поширення для шару згортки.\n",
        "\n",
        "    Параметри:\n",
        "    d_out - Градієнт вихідного тензора.\n",
        "    input_tensor - Вхідний тензор.\n",
        "    filters - Ваги фільтрів.\n",
        "    stride - Крок згортки.\n",
        "\n",
        "    Повертає:\n",
        "    d_input - Градієнт вхідного тензора.\n",
        "    d_filters - Градієнт ваг фільтрів.\n",
        "    d_biases - Градієнт зміщень фільтрів.\n",
        "    \"\"\"\n",
        "    (num_filters, num_channels, filter_size, _) = filters.shape\n",
        "    (_, input_dim, _) = input_tensor.shape\n",
        "\n",
        "    d_input = np.zeros(input_tensor.shape)  # Ініціалізація градієнта вхідного тензора\n",
        "    d_filters = np.zeros(filters.shape)  # Ініціалізація градієнта ваг фільтрів\n",
        "    d_biases = np.zeros((num_filters, 1))  # Ініціалізація градієнта зміщень фільтрів\n",
        "\n",
        "    for filter_index in range(num_filters):\n",
        "        input_y = output_y = 0\n",
        "        while input_y + filter_size <= input_dim:\n",
        "            input_x = output_x = 0\n",
        "            while input_x + filter_size <= input_dim:\n",
        "                # Обчислення градієнту ваг фільтрів\n",
        "                d_filters[filter_index] += d_out[filter_index, output_y, output_x] * input_tensor[:, input_y:input_y+filter_size, input_x:input_x+filter_size]\n",
        "                # Обчислення градієнту вхідного тензора\n",
        "                d_input[:, input_y:input_y+filter_size, input_x:input_x+filter_size] += d_out[filter_index, output_y, output_x] * filters[filter_index]\n",
        "                input_x += stride\n",
        "                output_x += 1\n",
        "            input_y += stride\n",
        "            output_y += 1\n",
        "        # Обчислення градієнту зміщень фільтрів\n",
        "        d_biases[filter_index] = np.sum(d_out[filter_index])\n",
        "\n",
        "    return d_input, d_filters, d_biases"
      ],
      "metadata": {
        "id": "Hn0ypxMg-LyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba0Sh9Pe3XwB"
      },
      "outputs": [],
      "source": [
        "#original\n",
        "def nanArgMax(arr):\n",
        "    idx = np.nanargmax(arr)\n",
        "    idxs = np.unravel_index(idx, arr.shape)\n",
        "    return idxs\n",
        "\n",
        "def maxpoolBackward(dpool, orig, f, s):\n",
        "    (n_c, orig_dim, _) = orig.shape\n",
        "\n",
        "    dout = np.zeros(orig.shape)\n",
        "\n",
        "    for curr_c in range(n_c):\n",
        "        curr_y = 0\n",
        "        out_y = 0\n",
        "        while curr_y + f <= orig_dim:\n",
        "            curr_x = 0\n",
        "            out_x = 0\n",
        "            while curr_x + f <= orig_dim:\n",
        "                (a, b) = nanArgMax(orig[curr_c, curr_y:curr_y+f, curr_x:curr_x+f])\n",
        "                dout[curr_c, curr_y+a, curr_x+b] += dpool[curr_c, out_y, out_x]\n",
        "\n",
        "                curr_x += s\n",
        "                out_x += 1\n",
        "            curr_y += s\n",
        "            out_y += 1\n",
        "\n",
        "    return dout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nan_arg_max(array):\n",
        "    # Знаходить позицію максимального значення у масиві, ігноруючи значення NaN.\n",
        "    index = np.nanargmax(array)  # Знаходить індекс максимального значення у масиві\n",
        "    indices = np.unravel_index(index, array.shape)  # Перетворює лінійний індекс у вимірні індекси\n",
        "    return indices  # Повертає позицію максимального значення у масиві\n",
        "\n",
        "def maxpool_backward(d_out, input_tensor, filter_size, stride):\n",
        "    # Обчислює зворотнє поширення для шару максимального пулінгу.\n",
        "    (num_channels, input_dim, _) = input_tensor.shape\n",
        "    d_input = np.zeros(input_tensor.shape)  # Ініціалізує градієнт вхідного тензора\n",
        "\n",
        "    for channel_index in range(num_channels):\n",
        "        input_y = output_y = 0\n",
        "        while input_y + filter_size <= input_dim:\n",
        "            input_x = output_x = 0\n",
        "            while input_x + filter_size <= input_dim:\n",
        "                # Знаходить позицію максимального значення у підмасиві\n",
        "                (max_y, max_x) = nan_arg_max(input_tensor[channel_index, input_y:input_y+filter_size, input_x:input_x+filter_size])\n",
        "                # Додає градієнт до відповідного вхідного пікселя\n",
        "                d_input[channel_index, input_y + max_y, input_x + max_x] += d_out[channel_index, output_y, output_x]\n",
        "                input_x += stride\n",
        "                output_x += 1\n",
        "            input_y += stride\n",
        "            output_y += 1\n",
        "    return d_input  # Повертає градієнт вхідного тензора"
      ],
      "metadata": {
        "id": "hDwMVNGv-Z6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOYRaiQkDBZE"
      },
      "outputs": [],
      "source": [
        "@njit(fastmath=True)\n",
        "def update_func(velocity, squared_gradient, beta1, beta2, batch_size, gradient, parameter, learning_rate):\n",
        "    \"\"\"\n",
        "    Оновлює параметр моделі за допомогою методу Adam.\n",
        "\n",
        "    Параметри:\n",
        "    velocity - Експоненційно зважене середнє градієнтів за минулі кроки.\n",
        "    squared_gradient - Експоненційно зважене середнє квадратів градієнтів за минулі кроки.\n",
        "    beta1 - Параметр згладжування для velocity.\n",
        "    beta2 - Параметр згладжування для squared_gradient.\n",
        "    batch_size - Розмір пакета даних.\n",
        "    gradient - Градієнт.\n",
        "    parameter - Параметр, який потрібно оновити.\n",
        "    learning_rate - Швидкість навчання.\n",
        "\n",
        "    Повертає:\n",
        "    np.array - Оновлений параметр моделі.\n",
        "    \"\"\"\n",
        "\n",
        "    velocity = beta1 * velocity + (1 - beta1) * gradient / batch_size  # Оновлюємо velocity за допомогою експоненційно зваженого середнього градієнтів\n",
        "    squared_gradient = beta2 * squared_gradient + (1 - beta2) * (gradient / batch_size) ** 2  # Оновлюємо squared_gradient за допомогою експоненційно зваженого середнього квадратів градієнтів\n",
        "    parameter -= learning_rate * velocity / np.sqrt(squared_gradient + 1e-7)  # Оновлюємо параметр parameter згідно формули Adam\n",
        "    return parameter  # Повертаємо оновлений параметр parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07D63kGrr80e"
      },
      "outputs": [],
      "source": [
        "def adam(params, grads, learning_rate, batch_size, beta1, beta2):\n",
        "    \"\"\"\n",
        "    Оновлює параметри за допомогою алгоритму Adam.\n",
        "\n",
        "    Параметри:\n",
        "    params - Список параметрів моделі.\n",
        "    grads - Список градієнтів моделі.\n",
        "    learning_rate - Швидкість навчання.\n",
        "    batch_size - Розмір пакета даних.\n",
        "    beta1 - Параметр згладжування для оновлення швидкості.\n",
        "    beta2 - Параметр згладжування для оновлення квадрату градієнту.\n",
        "\n",
        "    Повертає:\n",
        "    list - Оновлені параметри моделі.\n",
        "    \"\"\"\n",
        "    # Розпаковуємо параметри\n",
        "    kernel_conv1, bias_conv1, kernel_conv2, bias_conv2, kernel_conv3, bias_conv3, kernel_conv4, bias_conv4, kernel_conv5, bias_conv5, weights_f6, bias_f6, weights_f7, bias_f7, weights_output, bias_output = params\n",
        "\n",
        "    # Ініціалізуємо змінні для velocity\n",
        "    v1  = np.zeros(kernel_conv1.shape)\n",
        "    bv1 = np.zeros(bias_conv1.shape)\n",
        "    v2  = np.zeros(kernel_conv2.shape)\n",
        "    bv2 = np.zeros(bias_conv2.shape)\n",
        "    v3  = np.zeros(kernel_conv3.shape)\n",
        "    bv3 = np.zeros(bias_conv3.shape)\n",
        "    v4  = np.zeros(kernel_conv4.shape)\n",
        "    bv4 = np.zeros(bias_conv4.shape)\n",
        "    v5  = np.zeros(kernel_conv5.shape)\n",
        "    bv5 = np.zeros(bias_conv5.shape)\n",
        "    v6  = np.zeros(weights_f6.shape)\n",
        "    bv6 = np.zeros(bias_f6.shape)\n",
        "    v7  = np.zeros(weights_f7.shape)\n",
        "    bv7 = np.zeros(bias_f7.shape)\n",
        "    v8  = np.zeros(weights_output.shape)\n",
        "    bv8 = np.zeros(bias_output.shape)\n",
        "\n",
        "    # Ініціалізуємо змінні для squared_gradient\n",
        "    s1  = np.zeros(kernel_conv1.shape)\n",
        "    bs1 = np.zeros(bias_conv1.shape)\n",
        "    s2  = np.zeros(kernel_conv2.shape)\n",
        "    bs2 = np.zeros(bias_conv2.shape)\n",
        "    s3  = np.zeros(kernel_conv3.shape)\n",
        "    bs3 = np.zeros(bias_conv3.shape)\n",
        "    s4  = np.zeros(kernel_conv4.shape)\n",
        "    bs4 = np.zeros(bias_conv4.shape)\n",
        "    s5  = np.zeros(kernel_conv5.shape)\n",
        "    bs5 = np.zeros(bias_conv5.shape)\n",
        "    s6  = np.zeros(weights_f6.shape)\n",
        "    bs6 = np.zeros(bias_f6.shape)\n",
        "    s7  = np.zeros(weights_f7.shape)\n",
        "    bs7 = np.zeros(bias_f7.shape)\n",
        "    s8  = np.zeros(weights_output.shape)\n",
        "    bs8 = np.zeros(bias_output.shape)\n",
        "\n",
        "    # Розпаковуємо градієнти\n",
        "    d_kernel_conv1, d_bias_conv1, d_kernel_conv2, d_bias_conv2, d_kernel_conv3, d_bias_conv3, d_kernel_conv4, d_bias_conv4, d_kernel_conv5, d_bias_conv5, d_weights_f6, d_bias_f6, d_weights_f7, d_bias_f7, d_weights_output, d_bias_output = grads\n",
        "\n",
        "    # Оновлюємо кожен параметр за допомогою методу Adam\n",
        "    kernel_conv1 = update_func(v1, s1, beta1, beta2, batch_size, d_kernel_conv1, kernel_conv1, learning_rate)\n",
        "    bias_conv1 = update_func(bv1, bs1, beta1, beta2, batch_size, d_bias_conv1, bias_conv1, learning_rate)\n",
        "    kernel_conv2 = update_func(v2, s2, beta1, beta2, batch_size, d_kernel_conv2, kernel_conv2, learning_rate)\n",
        "    bias_conv2 = update_func(bv2, bs2, beta1, beta2, batch_size, d_bias_conv2, bias_conv2, learning_rate)\n",
        "    kernel_conv3 = update_func(v3, s3, beta1, beta2, batch_size, d_kernel_conv3, kernel_conv3, learning_rate)\n",
        "    bias_conv3 = update_func(bv3, bs3, beta1, beta2, batch_size, d_bias_conv3, bias_conv3, learning_rate)\n",
        "    kernel_conv4 = update_func(v4, s4, beta1, beta2, batch_size, d_kernel_conv4, kernel_conv4, learning_rate)\n",
        "    bias_conv4 = update_func(bv4, bs4, beta1, beta2, batch_size, d_bias_conv4, bias_conv4, learning_rate)\n",
        "    kernel_conv5 = update_func(v5, s5, beta1, beta2, batch_size, d_kernel_conv5, kernel_conv5, learning_rate)\n",
        "    bias_conv5 = update_func(bv5, bs5, beta1, beta2, batch_size, d_bias_conv5, bias_conv5, learning_rate)\n",
        "    weights_f6 = update_func(v6, s6, beta1, beta2, batch_size, d_weights_f6, weights_f6, learning_rate)\n",
        "    bias_f6 = update_func(bv6, bs6, beta1, beta2, batch_size, d_bias_f6, bias_f6, learning_rate)\n",
        "    weights_f7 = update_func(v7, s7, beta1, beta2, batch_size, d_weights_f7, weights_f7, learning_rate)\n",
        "    bias_f7 = update_func(bv7, bs7, beta1, beta2, batch_size, d_bias_f7, bias_f7, learning_rate)\n",
        "    weights_output = update_func(v8, s8, beta1, beta2, batch_size, d_weights_output, weights_output, learning_rate)\n",
        "    bias_output = update_func(bv8, bs8, beta1, beta2, batch_size, d_bias_output, bias_output, learning_rate)\n",
        "\n",
        "    # Групуємо оновлені параметри\n",
        "    params = [kernel_conv1, bias_conv1, kernel_conv2, bias_conv2, kernel_conv3, bias_conv3, kernel_conv4, bias_conv4, kernel_conv5, bias_conv5, weights_f6, bias_f6, weights_f7, bias_f7, weights_output, bias_output]\n",
        "\n",
        "    # Повертаємо оновлені параметри\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twESJ-MRRJnf"
      },
      "outputs": [],
      "source": [
        "def my_model():\n",
        "    # Функція для прямого проходження мережі\n",
        "    def forward_propagation(image, params):\n",
        "    # Розпаковка параметрів для кожного шару мережі\n",
        "        kernel_conv1, bias_conv1, kernel_conv2, bias_conv2, kernel_conv3, bias_conv3, kernel_conv4, bias_conv4, kernel_conv5, bias_conv5, weights_f6, bias_f6, weights_f7, bias_f7, weights_output, bias_output = params\n",
        "\n",
        "        # Прямий прохід через перші п'ять шарів: згорткові та пулінгові шари\n",
        "        conv1_output = relu(convolution_layer(image, kernel_conv1, bias_conv1, stride=4))  # Застосування згорткового шару з ReLU активацією та зсувом\n",
        "        pool1_output = max_pooling(conv1_output)  # Пулінговий шар\n",
        "        conv2_output = relu(convolution_layer(pool1_output, kernel_conv2, bias_conv2, padding=2))  # Застосування згорткового шару з ReLU активацією та зсувом\n",
        "        pool2_output = max_pooling(conv2_output)  # Пулінговий шар\n",
        "        conv3_output = relu(convolution_layer(pool2_output, kernel_conv3, bias_conv3, padding=1))  # Застосування згорткового шару з ReLU активацією та зсувом\n",
        "        conv4_output = relu(convolution_layer(conv3_output, kernel_conv4, bias_conv4, padding=1))  # Застосування згорткового шару з ReLU активацією та зсувом\n",
        "        conv5_output = relu(convolution_layer(conv4_output, kernel_conv5, bias_conv5, padding=1))  # Застосування згорткового шару з ReLU активацією та зсувом\n",
        "        pool5_output = max_pooling(conv5_output)  # Пулінговий шар\n",
        "\n",
        "        # Підготовка вихідного вектора для повнозв'язаного шару\n",
        "        (nf2, dim2, _) = pool5_output.shape\n",
        "        fc = pool5_output.reshape((nf2 * dim2 * dim2, 1))\n",
        "\n",
        "        # Прямий прохід через повнозв'язаний шар з ReLU активацією\n",
        "        fc6_output = relu(weights_f6.dot(fc) + bias_f6)\n",
        "        fc7_output = relu(weights_f7.dot(fc6_output) + bias_f7)\n",
        "        output_value = softmax(weights_output.dot(fc7_output) + bias_output)\n",
        "\n",
        "        # Пакування результатів для подальшого використання в зворотному проходженні\n",
        "        params = [conv1_output, pool1_output, conv2_output, pool2_output, conv3_output, conv4_output, conv5_output, pool5_output, fc, fc6_output, fc7_output, output_value]\n",
        "\n",
        "        return params\n",
        "\n",
        "    # Функція для зворотного проходження мережі\n",
        "    def backward_propagation(image, target, learning_rate, params):\n",
        "        # Розпаковка параметрів\n",
        "        kernel_conv1, bias_conv1, kernel_conv2, bias_conv2, kernel_conv3, bias_conv3, kernel_conv4, bias_conv4, kernel_conv5, bias_conv5, weights_f6, bias_f6, weights_f7, bias_f7, weights_output, bias_output = params\n",
        "        # Прямий прохід для отримання необхідних результатів\n",
        "        conv1_output, pool1_output, conv2_output, pool2_output, conv3_output, conv4_output, conv5_output, pool5_output, fc, fc6_output, fc7_output, output_value = forward_propagation(image, params)\n",
        "        # Розрахунок втрат\n",
        "        loss = cross_entropy_loss(output_value, target)\n",
        "\n",
        "        # Обчислення градієнтів відносно ваг та зсувів кожного шару\n",
        "        d_out = output_value - target\n",
        "\n",
        "        # Градієнти для вихідного шару\n",
        "        d_weights_output = d_out.dot(fc7_output.T)\n",
        "        d_bias_output = np.sum(d_out, axis=1).reshape(bias_output.shape)\n",
        "\n",
        "        # Градієнти для повнозв'язаного шару 7\n",
        "        d_fc7_output = weights_output.T.dot(d_out)\n",
        "        d_fc7_output[fc7_output <= 0] = 0\n",
        "        d_weights_f7 = d_fc7_output.dot(fc6_output.T)\n",
        "        d_bias_f7 = np.sum(d_fc7_output, axis=1).reshape(bias_f7.shape)\n",
        "\n",
        "        # Градієнти для повнозв'язаного шару 6\n",
        "        d_fc6_output = weights_f7.T.dot(d_fc7_output)\n",
        "        d_fc6_output[fc6_output <= 0] = 0\n",
        "        d_weights_f6 = d_fc6_output.dot(fc.T)\n",
        "        d_bias_f6 = np.sum(d_fc6_output, axis=1).reshape(bias_f6.shape)\n",
        "\n",
        "        # Градієнти для згорткових шарів та їх параметрів\n",
        "        d_fc = weights_f6.T.dot(d_fc6_output)\n",
        "        d_pool5_output = d_fc.reshape(pool5_output.shape)\n",
        "\n",
        "        d_conv5_output = maxpoolBackward(d_pool5_output, conv5_output, f=3, s=2)\n",
        "        d_conv5_output[conv5_output <= 0] = 0\n",
        "\n",
        "        d_conv4_output, d_kernel_conv5, d_bias_conv5 = convolutionBackward(d_conv5_output, conv4_output, kernel_conv5, s=1)\n",
        "        d_conv4_output[conv4_output <= 0] = 0\n",
        "\n",
        "        d_conv3_output, d_kernel_conv4, d_bias_conv4 = convolutionBackward(d_conv4_output, conv3_output, kernel_conv4, s=1)\n",
        "        d_conv3_output[conv3_output <= 0] = 0\n",
        "\n",
        "        d_pool2_output, d_kernel_conv3, d_bias_conv3 = convolutionBackward(d_conv3_output, pool2_output, kernel_conv3, s=1)\n",
        "        d_pool2_output[pool2_output <= 0] = 0\n",
        "\n",
        "        d_conv2_output = maxpoolBackward(d_pool2_output, conv2_output, f=3, s=2)\n",
        "        d_conv2_output[conv2_output <= 0] = 0\n",
        "\n",
        "        d_pool1_output, d_kernel_conv2, d_bias_conv2 = convolutionBackward(d_conv2_output, pool1_output, kernel_conv2, s=1)\n",
        "        d_pool1_output[pool1_output <= 0] = 0\n",
        "\n",
        "        d_conv1_output = maxpoolBackward(d_pool1_output, conv1_output, f=3, s=2)\n",
        "        d_conv1_output[conv1_output <= 0] = 0\n",
        "\n",
        "        d_image, d_kernel_conv1, d_bias_conv1 = convolutionBackward(d_conv1_output, image, kernel_conv1, s=4)\n",
        "\n",
        "        # Підготовка градієнтів для подальшого використання\n",
        "        grads = [d_kernel_conv1, d_bias_conv1, d_kernel_conv2, d_bias_conv2, d_kernel_conv3, d_bias_conv3, d_kernel_conv4, d_bias_conv4, d_kernel_conv5, d_bias_conv5, d_weights_f6, d_bias_f6, d_weights_f7, d_bias_f7, d_weights_output, d_bias_output]\n",
        "\n",
        "        return grads\n",
        "\n",
        "    return forward_propagation, backward_propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCsd0VEpMjKc"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(shape):\n",
        "    # Обчислення стандартного відхилення за методом Ксав'є\n",
        "    stddev = np.sqrt(2.0 / np.prod(shape[:-1]))\n",
        "    # Ініціалізація ваг за допомогою випадкового розподілу з нормальним розподілом\n",
        "    return np.random.normal(scale=stddev, size=shape)\n",
        "\n",
        "# Кількість класів у задачі класифікації\n",
        "num_classes = 10\n",
        "\n",
        "# Ініціалізація параметрів для кожного згорткового шару та повнозв'язаного шару згідно методу Ксав'є\n",
        "\n",
        "# Перший згортковий шар\n",
        "kernel_conv1 = initialize_weights((96, 3, 11, 11))  # 96 фільтрів розміром 11x11 з трьома входами кольору\n",
        "bias_conv1 = np.zeros((kernel_conv1.shape[0], 1))  # Зсуви для кожного фільтра\n",
        "\n",
        "# Другий згортковий шар\n",
        "kernel_conv2 = initialize_weights((256, 96, 5, 5))  # 256 фільтрів розміром 5x5, кожен з 96 входів\n",
        "bias_conv2 = np.zeros((kernel_conv2.shape[0], 1))  # Зсуви для кожного фільтра\n",
        "\n",
        "# Третій згортковий шар\n",
        "kernel_conv3 = initialize_weights((384, 256, 3, 3))  # 384 фільтри розміром 3x3, кожен з 256 входів\n",
        "bias_conv3 = np.zeros((kernel_conv3.shape[0], 1))  # Зсуви для кожного фільтра\n",
        "\n",
        "# Четвертий згортковий шар\n",
        "kernel_conv4 = initialize_weights((384, 384, 3, 3))  # 384 фільтри розміром 3x3, кожен з 384 входів\n",
        "bias_conv4 = np.zeros((kernel_conv4.shape[0], 1))  # Зсуви для кожного фільтра\n",
        "\n",
        "# П'ятий згортковий шар\n",
        "kernel_conv5 = initialize_weights((256, 384, 3, 3))  # 256 фільтрів розміром 3x3, кожен з 384 входів\n",
        "bias_conv5 = np.zeros((kernel_conv5.shape[0], 1))  # Зсуви для кожного фільтра\n",
        "\n",
        "# Повнозв'язаний шар 6\n",
        "weights_f6 = initialize_weights((4096, 9216))  # Матриця ваг розміром 4096x9216\n",
        "bias_f6 = np.zeros((weights_f6.shape[0], 1))  # Вектор зсуву\n",
        "\n",
        "# Повнозв'язаний шар 7\n",
        "weights_f7 = initialize_weights((4096, 4096))  # Матриця ваг розміром 4096x4096\n",
        "bias_f7 = np.zeros((weights_f7.shape[0], 1))  # Вектор зсуву\n",
        "\n",
        "# Вихідний повнозв'язаний шар\n",
        "weights_output = initialize_weights((num_classes, 4096))  # Матриця ваг розміром 10x4096 (для 10 класів)\n",
        "bias_output = np.zeros((weights_output.shape[0], 1))  # Вектор зсуву\n",
        "\n",
        "# Створення списку параметрів для подальшого використання\n",
        "params = [kernel_conv1, bias_conv1, kernel_conv2, bias_conv2, kernel_conv3, bias_conv3,\n",
        "          kernel_conv4, bias_conv4, kernel_conv5, bias_conv5, weights_f6, bias_f6,\n",
        "          weights_f7, bias_f7, weights_output, bias_output]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx_PvAk-Nni4"
      },
      "outputs": [],
      "source": [
        "# Завантаження набору даних CIFAR-10\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Перетворення типу даних на float32\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Нормалізація даних шляхом ділення на 255\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# Перетворення міток на one-hot вектори\n",
        "Y_train = to_categorical(y_train, num_classes)\n",
        "Y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Вибір підмножини даних для ефективнішого навчання\n",
        "\n",
        "# Визначення розміру підмножини та обчислення кількості зразків для навчання\n",
        "subset_fraction = 0.01\n",
        "subset_size = int(len(X_train) * subset_fraction)\n",
        "\n",
        "# Вибір випадкової підмножини з навчальних даних\n",
        "subset_indices = np.random.choice(len(X_train), size=subset_size, replace=False)\n",
        "X_train_subset = X_train[subset_indices]\n",
        "Y_train_subset = Y_train[subset_indices]\n",
        "\n",
        "# Вибір випадкової підмножини з тестових даних\n",
        "subset_indices = np.random.choice(len(X_test), size=subset_size, replace=False)\n",
        "X_test_subset = X_test[subset_indices]\n",
        "Y_test_subset = Y_test[subset_indices]\n",
        "\n",
        "# Зміна розміру зображень для моделі AlexNet\n",
        "\n",
        "# Зміна розміру навчальних зображень\n",
        "resized_images_train = []\n",
        "for i in range(len(X_train_subset)):\n",
        "    # Зміна розміру та перетворення порядку каналів зображення\n",
        "    resized_images_train.append(np.transpose(cv2.resize(X_train_subset[i], (227, 227), interpolation=cv2.INTER_LINEAR), (2, 0, 1)))\n",
        "X_train_subset_resized = np.array(resized_images_train)\n",
        "\n",
        "# Зміна розміру тестових зображень\n",
        "resized_images_test = []\n",
        "for i in range(len(X_test_subset)):\n",
        "    # Зміна розміру та перетворення порядку каналів зображення\n",
        "    resized_images_test.append(np.transpose(cv2.resize(X_test_subset[i], (227, 227), interpolation=cv2.INTER_LINEAR), (2, 0, 1)))\n",
        "X_test_subset_resized = np.array(resized_images_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T6JxsRsH0L-"
      },
      "outputs": [],
      "source": [
        "def train_model(epochs, batch_size, learning_rate, beta1, beta2, params):\n",
        "    # Розпакування параметрів\n",
        "    kernel_conv1, bias_conv1, kernel_conv2, bias_conv2, kernel_conv3, bias_conv3, kernel_conv4, bias_conv4, kernel_conv5, bias_conv5, weights_f6, bias_f6, weights_f7, bias_f7, weights_output, bias_output = params\n",
        "\n",
        "    # Отримання функцій прямого та зворотного поширення\n",
        "    forward_propagation, backward_propagation = my_model()\n",
        "\n",
        "    # Списки для збереження точності та втрат\n",
        "    accuracy_list = []\n",
        "    loss_list = []\n",
        "\n",
        "    # Розділення навчальних даних на пакети\n",
        "    batches = [X_train_subset_resized[i:i+batch_size] for i in range(0, len(X_train_subset_resized), batch_size)]\n",
        "\n",
        "    # Початок великого циклу для кожної епохи\n",
        "    total_start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Цикл по пакетам\n",
        "        for i in range(len(batches)):\n",
        "            print(f\"batch: {i}\")\n",
        "            # Ініціалізація градієнтів для кожного пакету\n",
        "            d_kernel_conv1   = np.zeros(kernel_conv1.shape)\n",
        "            d_bias_conv1     = np.zeros(bias_conv1.shape)\n",
        "            d_kernel_conv2   = np.zeros(kernel_conv2.shape)\n",
        "            d_bias_conv2     = np.zeros(bias_conv2.shape)\n",
        "            d_kernel_conv3   = np.zeros(kernel_conv3.shape)\n",
        "            d_bias_conv3     = np.zeros(bias_conv3.shape)\n",
        "            d_kernel_conv4   = np.zeros(kernel_conv4.shape)\n",
        "            d_bias_conv4     = np.zeros(bias_conv4.shape)\n",
        "            d_kernel_conv5   = np.zeros(kernel_conv5.shape)\n",
        "            d_bias_conv5     = np.zeros(bias_conv5.shape)\n",
        "            d_weights_f6     = np.zeros(weights_f6.shape)\n",
        "            d_bias_f6        = np.zeros(bias_f6.shape)\n",
        "            d_weights_f7     = np.zeros(weights_f7.shape)\n",
        "            d_bias_f7        = np.zeros(bias_f7.shape)\n",
        "            d_weights_output = np.zeros(weights_output.shape)\n",
        "            d_bias_output    = np.zeros(bias_output.shape)\n",
        "\n",
        "            # Обчислення градієнтів за допомогою зворотного поширення для кожного зразка в пакеті\n",
        "            for k in range(len(batches[i])):\n",
        "                d_kernel_conv1_, d_bias_conv1_, d_kernel_conv2_, d_bias_conv2_, d_kernel_conv3_, d_bias_conv3_, d_kernel_conv4_, d_bias_conv4_, d_kernel_conv5_, d_bias_conv5_, d_weights_f6_, d_bias_f6_, d_weights_f7_, d_bias_f7_, d_weights_output_, d_bias_output_ = backward_propagation(batches[i][k], Y_train_subset[batch_size*i+k].reshape(-1, 1), learning_rate, params)\n",
        "                d_kernel_conv1   += d_kernel_conv1_\n",
        "                d_bias_conv1     += d_bias_conv1_\n",
        "                d_kernel_conv2   += d_kernel_conv2_\n",
        "                d_bias_conv2     += d_bias_conv2_\n",
        "                d_kernel_conv3   += d_kernel_conv3_\n",
        "                d_bias_conv3     += d_bias_conv3_\n",
        "                d_kernel_conv4   += d_kernel_conv4_\n",
        "                d_bias_conv4     += d_bias_conv4_\n",
        "                d_kernel_conv5   += d_kernel_conv5_\n",
        "                d_bias_conv5     += d_bias_conv5_\n",
        "                d_weights_f6     += d_weights_f6_\n",
        "                d_bias_f6        += d_bias_f6_\n",
        "                d_weights_f7     += d_weights_f7_\n",
        "                d_bias_f7        += d_bias_f7_\n",
        "                d_weights_output += d_weights_output_\n",
        "                d_bias_output    += d_bias_output_\n",
        "\n",
        "            # Збереження градієнтів\n",
        "            grads = [d_kernel_conv1, d_bias_conv1, d_kernel_conv2, d_bias_conv2, d_kernel_conv3, d_bias_conv3, d_kernel_conv4, d_bias_conv4, d_kernel_conv5, d_bias_conv5, d_weights_f6, d_bias_f6, d_weights_f7, d_bias_f7, d_weights_output, d_bias_output]\n",
        "            params = [kernel_conv1, bias_conv1, kernel_conv2, bias_conv2, kernel_conv3, bias_conv3, kernel_conv4, bias_conv4, kernel_conv5, bias_conv5, weights_f6, bias_f6, weights_f7, bias_f7, weights_output, bias_output]\n",
        "            kernel_conv1, bias_conv1, kernel_conv2, bias_conv2, kernel_conv3, bias_conv3, kernel_conv4, bias_conv4, kernel_conv5, bias_conv5, weights_f6, bias_f6, weights_f7, bias_f7, weights_output, bias_output = adam(params, grads, learning_rate, len(batches[i]), beta1, beta2)\n",
        "        # Обчислення часу, який зайняла епоха\n",
        "        end_time = time.time()\n",
        "        hours, remainder = divmod(end_time-start_time, 3600)\n",
        "        minutes, seconds = divmod(remainder, 60)\n",
        "        # Оцінка точності та втрат для валідаційних даних\n",
        "        predicted_labels = []\n",
        "        for i in range(len(X_test_subset_resized)):\n",
        "            image = X_test_subset_resized[i]\n",
        "            label = Y_test_subset[i]\n",
        "            output = forward_propagation(image, params)[11]\n",
        "            loss = cross_entropy_loss(output, label)\n",
        "            epoch_loss += loss\n",
        "            predicted_label = np.argmax(output)\n",
        "            predicted_labels.append(predicted_label)\n",
        "        # Обчислення середньої втрати за епоху\n",
        "        avg_epoch_loss = epoch_loss / len(X_test_subset_resized)\n",
        "        # Додавання середньої втрати до списку втрат для подальшого аналізу\n",
        "        loss_list.append(avg_epoch_loss)\n",
        "        # Передбачення міток для валідаційних даних та оцінка точності\n",
        "        predicted_labels = np.array(predicted_labels)\n",
        "        true_labels = np.argmax(Y_test_subset, axis=1)\n",
        "        accuracy = np.mean(predicted_labels == true_labels)\n",
        "        # Додавання точності до списку точності для подальшого аналізу\n",
        "        accuracy_list.append(accuracy)\n",
        "        # Вивід інформації про поточну епоху, час її завершення, точність та середню втрату\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} - Time: {int(hours)}h:{int(minutes)}m:{int(seconds)}s - Accuracy: {accuracy} - Loss: {avg_epoch_loss}\")\n",
        "    # Отримання часу завершення всього тренування\n",
        "    total_end_time = time.time()\n",
        "    # Розрахунок часу, який пройшов від початку до завершення тренування в годинах, хвилинах та секундах\n",
        "    t_hours, t_remainder = divmod(total_end_time-total_start_time, 3600)\n",
        "    t_minutes, t_seconds = divmod(t_remainder, 60)\n",
        "    # Виведення загального часу тренування\n",
        "    print(f\"Total time: {int(t_hours)}h:{int(t_minutes)}m:{int(t_seconds)}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXbcPbiSqfmp"
      },
      "outputs": [],
      "source": [
        "epochs = 10  # Кількість епох тренування\n",
        "batch_size = 16  # Розмір пакета для навчання\n",
        "learning_rate = 0.0001  # Швидкість навчання\n",
        "beta1 = 0.95  # Параметр бета1 для оптимізатора Adam\n",
        "beta2 = 0.99  # Параметр бета2 для оптимізатора Adam\n",
        "\n",
        "# Виклик функції тренування моделі зі встановленими параметрами\n",
        "train_model(epochs, batch_size, learning_rate, beta1, beta2, params)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}