{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwYwvP9P3fLZ"
      },
      "outputs": [],
      "source": [
        "# Завантаження бібліотек\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(777)"
      ],
      "metadata": {
        "id": "-Kox-IaB8X5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функція активації ReLU\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)"
      ],
      "metadata": {
        "id": "aXsB3Cvmrsjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функція активації Softmax\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "dRwWmaADIOTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Шар згортки\n",
        "def convolution_layer(input_value, kernel, bias, stride=1, padding=0):\n",
        "    # Визначення розмірів вхідного значення\n",
        "    input_depth, input_height, input_width = input_value.shape\n",
        "    # Визначення розмірів ядра\n",
        "    kernel_depth, kernel_height, kernel_width = kernel.shape\n",
        "    # Розрахунок висоти та ширини вихідного значення\n",
        "    output_height = (input_height - kernel_height + 2 * padding) // stride + 1\n",
        "    output_width = (input_width - kernel_width + 2 * padding) // stride + 1\n",
        "    # Ініціалізація вихідного значення нулями\n",
        "    output_value = np.zeros((kernel_depth, output_height, output_width))\n",
        "    # Додавання паддінгу до вхідного значення\n",
        "    padded_input = np.pad(input_value, ((0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
        "\n",
        "    # Перебір по всіх глибинах ядра\n",
        "    for d in range(kernel_depth):\n",
        "        # Перебір по висоті вхідного значення з урахуванням кроку\n",
        "        for i in range(0, input_height - kernel_height + 1, stride):\n",
        "            # Перебір по ширині вхідного значення з урахуванням кроку\n",
        "            for j in range(0, input_width - kernel_width + 1, stride):\n",
        "                # Обчислення значення на виході через згортку та додавання зміщення\n",
        "                output_value[d, i // stride, j // stride] = np.sum(\n",
        "                    padded_input[:, i:i + kernel_height, j:j + kernel_width] * kernel[d]) + bias[d]\n",
        "    # Повернення вихідного значення\n",
        "    return output_value"
      ],
      "metadata": {
        "id": "kt066_YXu7on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Шар максимального пулінгу\n",
        "def max_pooling(input_value, pool_size=2, stride=2):\n",
        "    # Визначення розмірів вхідного значення\n",
        "    input_depth, input_height, input_width = input_value.shape\n",
        "    # Розрахунок висоти та ширини вихідного значення\n",
        "    output_height = (input_height - pool_size) // stride + 1\n",
        "    output_width = (input_width - pool_size) // stride + 1\n",
        "    # Ініціалізація вихідного значення нулями\n",
        "    output_value = np.zeros((input_depth, output_height, output_width))\n",
        "\n",
        "    # Перебір по всіх глибинах вхідного значення\n",
        "    for d in range(input_depth):\n",
        "        # Перебір по висоті вхідного значення з урахуванням кроку\n",
        "        for i in range(0, input_height - pool_size + 1, stride):\n",
        "            # Перебір по ширині вхідного значення з урахуванням кроку\n",
        "            for j in range(0, input_width - pool_size + 1, stride):\n",
        "                # Обчислення максимального значення в поточному підвікні\n",
        "                output_value[d, i // stride, j // stride] = np.max(\n",
        "                    input_value[d, i:i + pool_size, j:j + pool_size])\n",
        "    # Повернення вихідного значення\n",
        "    return output_value"
      ],
      "metadata": {
        "id": "-SYtkPn4pNBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Повнозв'язний шар\n",
        "def fully_connected_layer(input_value, weights, bias):\n",
        "    output_value = np.dot(input_value.flatten(), weights) + bias\n",
        "    return output_value"
      ],
      "metadata": {
        "id": "J2LueQKZH3M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функція втрат - категорійна крос-ентропія\n",
        "def cross_entropy_loss(predicted, target):\n",
        "    return -np.sum(target * np.log(predicted))"
      ],
      "metadata": {
        "id": "eKEQ_iuuCS-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Шар Dropout\n",
        "def dropout_layer(input_value, dropout_rate):\n",
        "    dropout_mask = np.random.rand(*input_value.shape) < (1 - dropout_rate)\n",
        "    return input_value * dropout_mask"
      ],
      "metadata": {
        "id": "BSm1PmkFwzUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_model():\n",
        "    def forward_propagation(image, stride=2, pool_size=2):\n",
        "        # Встановлення ймовірності dropout\n",
        "        dropout_rate = 0.25\n",
        "\n",
        "        # Пропуск через перший шар згортки та функцію активації ReLU\n",
        "        conv1_output = relu(convolution_layer(image, kernel_conv1, bias_conv1))\n",
        "        # Застосування Dropout layer\n",
        "        conv1_output = dropout_layer(conv1_output, dropout_rate)\n",
        "        # Застосування шару Max Pooling\n",
        "        pool1_output = max_pooling(conv1_output)\n",
        "\n",
        "        # Пропуск через другий шар згортки та функцію активації ReLU\n",
        "        conv2_output = relu(convolution_layer(pool1_output, kernel_conv2, bias_conv2))\n",
        "        # Застосування Dropout layer\n",
        "        conv2_output = dropout_layer(conv2_output, dropout_rate)\n",
        "        # Застосування шару Max Pooling з заданим розміром вікна та кроком\n",
        "        pool2_output = max_pooling(conv2_output, pool_size=pool_size, stride=stride)\n",
        "\n",
        "        # Пропуск через третій шар згортки та функцію активації ReLU\n",
        "        conv3_output = relu(convolution_layer(pool2_output, kernel_conv3, bias_conv3))\n",
        "\n",
        "        # Пропуск через повнозв'язний шар\n",
        "        f6_output = fully_connected_layer(conv3_output, weights_f6, bias_f6)\n",
        "        # Пропуск через шар softmax для отримання ймовірностей класів\n",
        "        output_value = softmax(fully_connected_layer(f6_output, weights_output, bias_output).reshape(1, -1))\n",
        "\n",
        "        return conv1_output, pool1_output, conv2_output, pool2_output, conv3_output, f6_output, output_value\n",
        "\n",
        "    def backward_propagation(image, target, learning_rate, stride=2, pool_size=2):\n",
        "        # Використання глобальних змінних для параметрів моделі\n",
        "        global kernel_conv1, bias_conv1, kernel_conv2, bias_conv2, kernel_conv3, bias_conv3, weights_f6, bias_f6, weights_output, bias_output\n",
        "\n",
        "        # Виконання прямого поширення\n",
        "        conv1_output, pool1_output, conv2_output, pool2_output, conv3_output, f6_output, output_value = forward_propagation(image, stride, pool_size)\n",
        "\n",
        "        # Обчислення похибки на виході (delta) як різниця між виходом і цільовим значенням\n",
        "        delta_output = output_value - target\n",
        "        # Обчислення градієнтів для ваг і зміщень вихідного шару\n",
        "        grad_weights_output = np.outer(f6_output, delta_output)\n",
        "        grad_bias_output = delta_output\n",
        "\n",
        "        # Обчислення похибки для попереднього шару\n",
        "        delta_f6 = np.dot(delta_output, weights_output.T)\n",
        "        delta_conv3 = np.dot(delta_f6, weights_f6.T).reshape(conv3_output.shape)\n",
        "\n",
        "        # Обчислення градієнтів для ваг і зміщень повнозв'язного шару\n",
        "        grad_weights_f6 = np.outer(conv3_output.flatten(), delta_f6)\n",
        "        grad_bias_f6 = delta_f6\n",
        "\n",
        "        # Обчислення похибки для шару Max Pooling 2\n",
        "        delta_pool2 = np.zeros_like(pool2_output)\n",
        "        for d in range(pool2_output.shape[0]):\n",
        "            for i in range(0, pool2_output.shape[1], stride):\n",
        "                for j in range(0, pool2_output.shape[2], stride):\n",
        "                    if (i // stride < delta_conv3.shape[1]) and (j // stride < delta_conv3.shape[2]):\n",
        "                        window = pool2_output[d, i:i + pool_size, j:j + pool_size]\n",
        "                        mask = window == np.max(window)\n",
        "                        delta_pool2[d, i:i + pool_size, j:j + pool_size] = mask * delta_conv3[d, i // stride, j // stride]\n",
        "\n",
        "        # Обчислення похибки для другого шару згортки\n",
        "        delta_conv2 = np.zeros_like(conv2_output)\n",
        "        for d in range(conv2_output.shape[0]):\n",
        "            for i in range(conv2_output.shape[1]):\n",
        "                for j in range(conv2_output.shape[2]):\n",
        "                    if (i // stride < delta_pool2.shape[1]) and (j // stride < delta_pool2.shape[2]):\n",
        "                        delta_conv2[d, i, j] = np.sum(delta_pool2[d, i:i + stride, j:j + stride])\n",
        "\n",
        "        # Використання функції активації ReLU для похибки\n",
        "        delta_conv2 *= (conv2_output > 0)\n",
        "\n",
        "        # Обчислення градієнтів для третього шару згортки\n",
        "        grad_kernel_conv3 = convolution_layer(pool2_output, delta_conv3, np.zeros_like(bias_conv3), stride=1, padding=0)\n",
        "        grad_bias_conv3 = np.sum(delta_conv3, axis=(1, 2))\n",
        "\n",
        "        # Обчислення похибки для першого шару Max Pooling\n",
        "        delta_pool1 = np.zeros_like(pool1_output)\n",
        "        for d in range(pool1_output.shape[0]):\n",
        "            for i in range(0, pool1_output.shape[1], stride):\n",
        "                for j in range(0, pool1_output.shape[2], stride):\n",
        "                    if (i // stride < delta_conv2.shape[1]) and (j // stride < delta_conv2.shape[2]):\n",
        "                        window = pool1_output[d, i:i + pool_size, j:j + pool_size]\n",
        "                        mask = window == np.max(window)\n",
        "                        delta_pool1[d, i:i + pool_size, j:j + pool_size] = mask * delta_conv2[d, i // stride, j // stride]\n",
        "\n",
        "        # Обчислення похибки для першого шару згортки, використовуючи збільшення похибки\n",
        "        delta_conv1 = np.kron(delta_pool1, np.ones((1, stride, stride)))\n",
        "\n",
        "        # Використання функції активації ReLU для похибки\n",
        "        delta_conv1 *= (conv1_output > 0)\n",
        "\n",
        "        # Обчислення градієнтів для другого шару згортки\n",
        "        grad_kernel_conv2 = convolution_layer(pool1_output, delta_conv2, np.zeros_like(bias_conv2), stride=1, padding=0)\n",
        "        grad_bias_conv2 = np.sum(delta_conv2, axis=(1, 2))\n",
        "\n",
        "        # Обчислення градієнтів для першого шару згортки\n",
        "        grad_kernel_conv1 = convolution_layer(image, delta_conv1, np.zeros_like(bias_conv1), stride=1, padding=0)\n",
        "        grad_bias_conv1 = np.sum(delta_conv1, axis=(1, 2))\n",
        "\n",
        "        # Перетворення градієнтів зміщень у вектор\n",
        "        grad_bias_f6 = grad_bias_f6.reshape(-1)\n",
        "        grad_bias_output = grad_bias_output.reshape(-1)\n",
        "\n",
        "        # Оновлення параметрів моделі за допомогою градієнтів та швидкості навчання\n",
        "        kernel_conv1 -= learning_rate * grad_kernel_conv1\n",
        "        bias_conv1 -= learning_rate * grad_bias_conv1\n",
        "        kernel_conv2 -= learning_rate * grad_kernel_conv2\n",
        "        bias_conv2 -= learning_rate * grad_bias_conv2\n",
        "        kernel_conv3 -= learning_rate * grad_kernel_conv3\n",
        "        bias_conv3 -= learning_rate * grad_bias_conv3\n",
        "        weights_f6 -= learning_rate * grad_weights_f6\n",
        "        bias_f6 -= learning_rate * grad_bias_f6\n",
        "        weights_output -= learning_rate * grad_weights_output\n",
        "        bias_output -= learning_rate * grad_bias_output\n",
        "\n",
        "    return forward_propagation, backward_propagation"
      ],
      "metadata": {
        "id": "13hEBkqG5VE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функція для ініціалізації вагів за методом Ксав'є\n",
        "def initialize_weights(shape):\n",
        "    stddev = np.sqrt(2.0 / np.prod(shape[:-1]))\n",
        "    return np.random.normal(scale=stddev, size=shape)\n",
        "\n",
        "# Функція для ініціалізації біасу нулями\n",
        "def initialize_bias(shape):\n",
        "    return np.zeros(shape)\n",
        "\n",
        "# Кількість класів\n",
        "num_classes = 10\n",
        "# Ініціалізація вагових коефіцієнтів і зміщень для першого шару згортки\n",
        "kernel_conv1 = initialize_weights((6, 5, 5))\n",
        "bias_conv1 = initialize_bias(6)\n",
        "# Ініціалізація вагових коефіцієнтів і зміщень для другого шару згортки\n",
        "kernel_conv2 = initialize_weights((16, 5, 5))\n",
        "bias_conv2 = initialize_bias(16)\n",
        "# Ініціалізація вагових коефіцієнтів і зміщень для третього шару згортки\n",
        "kernel_conv3 = initialize_weights((120, 5, 5))\n",
        "bias_conv3 = initialize_bias(120)\n",
        "# Ініціалізація вагових коефіцієнтів і зміщень для повнозв'язного шару\n",
        "weights_f6 = initialize_weights((120, 84))\n",
        "bias_f6 = initialize_bias(84)\n",
        "# Ініціалізація вагових коефіцієнтів і зміщень для вихідного шару\n",
        "weights_output = initialize_weights((84, num_classes))\n",
        "bias_output = initialize_bias(num_classes)"
      ],
      "metadata": {
        "id": "--qfgFetuJhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Завантаження набору даних MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Перетворення типу даних зображень на float32\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Нормалізація зображень шляхом ділення на 255\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# Зміна розміру зображень та додавання додаткового виміру для каналу\n",
        "X_train_resize = np.expand_dims(np.pad(X_train, ((0,0), (2,2), (2,2)), mode='constant'), axis=1)\n",
        "X_test_resize = np.expand_dims(np.pad(X_test, ((0,0), (2,2), (2,2)), mode='constant'), axis=1)\n",
        "\n",
        "# Перетворення міток на one-hot вектори\n",
        "Y_train = to_categorical(y_train, num_classes)\n",
        "Y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Вибір підмножини даних для тренування\n",
        "subset_fraction = 0.02  # Фракція підмножини\n",
        "subset_size = int(len(X_train) * subset_fraction)  # Розмір підмножини\n",
        "subset_indices = np.random.choice(len(X_train), size=subset_size, replace=False)  # Випадковий вибір індексів\n",
        "X_train_subset = X_train_resize[subset_indices]  # Підмножина тренувальних зображень\n",
        "Y_train_subset = Y_train[subset_indices]  # Підмножина тренувальних міток\n",
        "\n",
        "# Вибір підмножини даних для тестування\n",
        "subset_size = int(len(X_test) * subset_fraction)  # Розмір підмножини\n",
        "subset_indices = np.random.choice(len(X_test), size=subset_size, replace=False)  # Випадковий вибір індексів\n",
        "X_test_subset = X_test_resize[subset_indices]  # Підмножина тестових зображень\n",
        "Y_test_subset = Y_test[subset_indices]  # Підмножина тестових міток"
      ],
      "metadata": {
        "id": "NjWgC6-NZrM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ініціалізація функцій прямого та зворотного поширення\n",
        "forward_propagation, backward_propagation = my_model()\n",
        "\n",
        "# Кількість епох для тренування моделі\n",
        "epochs = 1\n",
        "# Швидкість навчання\n",
        "learning_rate = 0.0001\n",
        "# Загальний час початку тренування\n",
        "total_start_time = time.time()\n",
        "\n",
        "# Списки для збереження точності та втрат\n",
        "accuracy_list = []\n",
        "loss_list = []\n",
        "\n",
        "# Цикл по епохам\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0  # Змінна для зберігання загальної втрати за епоху\n",
        "    start_time = time.time()  # Час початку епохи\n",
        "\n",
        "    # Цикл по всіх зображеннях тренувальної підмножини\n",
        "    for i in range(len(X_train_subset)):\n",
        "        image = X_train_subset[i]  # Вибір зображення\n",
        "        label = Y_train_subset[i]  # Вибір відповідної мітки\n",
        "        # Виконання зворотного поширення для оновлення ваг\n",
        "        backward_propagation(image, label, learning_rate)\n",
        "\n",
        "    end_time = time.time()  # Час завершення епохи\n",
        "    hours, remainder = divmod(end_time - start_time, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "    predicted_labels = []  # Список для зберігання передбачених міток\n",
        "\n",
        "    # Цикл по всіх зображеннях тестової підмножини\n",
        "    for i in range(len(X_test_subset)):\n",
        "        image = X_test_subset[i]  # Вибір зображення\n",
        "        label = Y_test_subset[i]  # Вибір відповідної мітки\n",
        "        # Отримання виходу моделі\n",
        "        output = forward_propagation(image)[6]\n",
        "        # Обчислення втрати з використанням функції cross_entropy_loss\n",
        "        loss = cross_entropy_loss(output, label)\n",
        "        epoch_loss += loss  # Додавання втрати до загальної втрати за епоху\n",
        "        predicted_label = np.argmax(output)  # Отримання передбаченої мітки\n",
        "        predicted_labels.append(predicted_label)  # Додавання передбаченої мітки до списку\n",
        "\n",
        "    # Обчислення середньої втрати за епоху\n",
        "    avg_epoch_loss = epoch_loss / len(X_test_subset)\n",
        "    loss_list.append(avg_epoch_loss)  # Додавання середньої втрати до списку втрат\n",
        "    predicted_labels = np.array(predicted_labels)\n",
        "    true_labels = np.argmax(Y_test_subset, axis=1)\n",
        "    # Обчислення точності\n",
        "    accuracy = np.mean(predicted_labels == true_labels)\n",
        "    accuracy_list.append(accuracy)  # Додавання точності до списку точностей\n",
        "    print(f\"Epoch: {epoch+1}/{epochs} - Time: {int(hours)}h:{int(minutes)}m:{int(seconds)}s - Accuracy: {accuracy} - Loss: {avg_epoch_loss}\")\n",
        "\n",
        "total_end_time = time.time()\n",
        "t_hours, t_remainder = divmod(total_end_time - total_start_time, 3600)\n",
        "t_minutes, t_seconds = divmod(t_remainder, 60)\n",
        "print(f\"Total time: {int(t_hours)}h:{int(t_minutes)}m:{int(t_seconds)}s\")"
      ],
      "metadata": {
        "id": "dIqPMhE_A_IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model"
      ],
      "metadata": {
        "id": "eIN8IRlj0k0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Імпорт бібліотеки для зберігання словника параметрів\n",
        "import pickle"
      ],
      "metadata": {
        "id": "9shwBPAK0yY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_parameters(parameters, filename):\n",
        "    \"\"\"\n",
        "    Зберігає параметри моделі у файл.\n",
        "\n",
        "    Аргументи:\n",
        "    parameters - словник, який містить параметри моделі (ваги, зміщення)\n",
        "    filename - ім'я файлу для збереження параметрів\n",
        "\n",
        "    Використання:\n",
        "    save_model_parameters(model_parameters, 'model_parameters.pkl')\n",
        "    \"\"\"\n",
        "    # Відкриваємо файл для запису в двійковому режимі\n",
        "    with open(filename, 'wb') as model_file:\n",
        "        # Зберігаємо словник параметрів у файл за допомогою pickle\n",
        "        pickle.dump(parameters, model_file)"
      ],
      "metadata": {
        "id": "kB3FaBSvFrTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Створення словника з параметрами моделі\n",
        "# Ключі словника є іменами параметрів, а значеннями є відповідні матриці ваг і зміщень\n",
        "\n",
        "parameters_to_save = {\n",
        "    # Параметри першого шару згортки\n",
        "    'kernel_conv1': kernel_conv1, # Вагові коефіцієнти першого шару згортки\n",
        "    'bias_conv1': bias_conv1, # Зміщення першого шару згортки\n",
        "    # Параметри другого шару згортки\n",
        "    'kernel_conv2': kernel_conv2, # Вагові коефіцієнти другого шару згортки\n",
        "    'bias_conv2': bias_conv2, # Зміщення другого шару згортки\n",
        "    # Параметри третього шару згортки\n",
        "    'kernel_conv3': kernel_conv3, # Вагові коефіцієнти третього шару згортки\n",
        "    'bias_conv3': bias_conv3, # Зміщення третього шару згортки\n",
        "    # Параметри повнозв'язного шару\n",
        "    'weights_f6': weights_f6, # Вагові коефіцієнти повнозв'язного шару\n",
        "    'bias_f6': bias_f6, # Зміщення повнозв'язного шару\n",
        "    # Параметри вихідного шару\n",
        "    'weights_output': weights_output, # Вагові коефіцієнти вихідного шару\n",
        "    'bias_output': bias_output # Зміщення вихідного шару\n",
        "}\n",
        "#Збереження параметрів моделі у файл 'my_model.pkl'\n",
        "save_model_parameters(parameters_to_save, 'my_model.pkl')"
      ],
      "metadata": {
        "id": "KIXQOsYI0vQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "YZhKj92Kr1tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Імпорт бібліотеки для візуалізації\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "jna10d4dqAbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Створення графіку точності на епохах\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(np.arange(1, epochs+1), accuracy_list, marker='o', linestyle='-')\n",
        "plt.title('Accuracy on Epoch') # Заголовок графіку\n",
        "plt.xlabel('Epoch') # Підпис осі X\n",
        "plt.ylabel('Accuracy') # Підпис осі Y\n",
        "plt.grid(True) # Включення сітки на графіку\n",
        "plt.show() # Відображення графіку"
      ],
      "metadata": {
        "id": "64ZOY-_0sAkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Створення графіку втрат на епохах\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(np.arange(1, epochs+1), loss_list, marker='o', linestyle='-', color='red')\n",
        "plt.title('Loss on Epoch') # Заголовок графіку\n",
        "plt.xlabel('Epoch') # Підпис осі X\n",
        "plt.ylabel('Loss') # Підпис осі Y\n",
        "plt.grid(True) # Включення сітки на графіку\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3Pl7gcHNsCO4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}